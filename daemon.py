import os
import time
import json
import re
import requests
import subprocess
import logging
import random
import html
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
from youtube_transcript_api import YouTubeTranscriptApi

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(r'c:\youtubehabit', 'daemon.log'), encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Configuration from .env
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

BASE_PATH = r'c:\youtubehabit'
WEBSITE_PATH = os.path.join(BASE_PATH, 'index.html')
SEEN_VIDEOS_FILE = os.path.join(BASE_PATH, 'seen_videos.json')
RECORDS_FILE = os.path.join(BASE_PATH, 'records.json')
SUBSCRIPTION_FILE = os.path.join(BASE_PATH, 'è¨‚é–±.txt')

if not DEEPSEEK_API_KEY:
    logging.error("DEEPSEEK_API_KEY not found in .env")
    exit(1)

client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")

def get_telegram_chat_id():
    """Attempt to get chat ID from the latest bot update."""
    global TELEGRAM_CHAT_ID
    if TELEGRAM_CHAT_ID:
        return TELEGRAM_CHAT_ID
    
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/getUpdates"
        resp = requests.get(url, timeout=10).json()
        if resp.get("ok") and resp.get("result"):
            # Get chat ID from the last message
            last_chat_id = resp["result"][-1]["message"]["chat"]["id"]
            logging.info(f"Automatically detected Telegram Chat ID: {last_chat_id}")
            return str(last_chat_id)
    except Exception as e:
        logging.error(f"Error getting Telegram Chat ID: {e}")
    return None

def send_telegram_msg(message):
    global TELEGRAM_CHAT_ID
    if not TELEGRAM_BOT_TOKEN:
        return
    
    if not TELEGRAM_CHAT_ID:
        TELEGRAM_CHAT_ID = get_telegram_chat_id()
    
    if not TELEGRAM_CHAT_ID:
        logging.warning("Telegram Chat ID not found. Please send a message to the bot first.")
        return

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }
    try:
        resp = requests.post(url, json=payload, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        logging.error(f"Failed to send Telegram message (HTML): {e}")
        if 'resp' in locals() and resp is not None:
             logging.error(f"Telegram response: {resp.text}")
        
        # Fallback to plain text
        try:
            payload['parse_mode'] = None
            resp = requests.post(url, json=payload, timeout=10)
            resp.raise_for_status()
            logging.info("Sent Telegram message using plain text fallback.")
        except Exception as e2:
            logging.error(f"Failed to send Telegram message (Plain Text): {e2}")

def save_json(filepath, data):
    try:
        temp_file = filepath + ".tmp"
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
        os.replace(temp_file, filepath)
    except Exception as e:
        logging.error(f"Error saving JSON to {filepath}: {e}")

def load_json(filepath, default):
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"Error loading JSON from {filepath}: {e}")
    return default

def get_subscriptions():
    """Load subscriptions from file and filter for relevant (Finance/Crypto) channels."""
    if not os.path.exists(SUBSCRIPTION_FILE):
        logging.warning(f"Subscription file not found: {SUBSCRIPTION_FILE}")
        return []
    
    try:
        with open(SUBSCRIPTION_FILE, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        # Finance/Crypto keywords to filter relevant channels
        keywords = [
            'è‚¡å¸‚', 'è²¡ç¶“', 'æŠ•è³‡', 'æ¯”ç‰¹å¹£', 'åŠ å¯†', 'ç¾è‚¡', 'æ¸¯è‚¡', 'å¹£åœˆ', 'é‡‘è', 
            'Bitcoin', 'Crypto', 'Stock', 'Market', 'BTC', 'ETH', 'ADA', 'XRP', 
            'åˆ†æ', 'è¡Œæƒ…', 'ç­–ç•¥', 'é‡‘', 'éŠ€', 'æ²¹', 'Money', 'Wealth', 'Trading', 
            'Invest', 'Finance', 'Economics', 'Dividend', 'Option', 'Future', 'Fund',
            'Business', 'Capital', 'Asset'
        ]
        
        relevant_subs = [
            sub for sub in lines 
            if any(kw.lower() in sub.lower() for kw in keywords)
        ]
        
        logging.info(f"Loaded {len(relevant_subs)} relevant subscriptions out of {len(lines)} total.")
        return relevant_subs
    except Exception as e:
        logging.error(f"Error loading subscriptions: {e}")
        return []

def get_transcript(video_id):
    try:
        # Try getting Chinese or English transcripts directly
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['zh-TW', 'zh-HK', 'zh-CN', 'en'])
        return " ".join([t['text'] for t in transcript])
    except Exception as e:
        try:
            # Fallback: List all available and find one
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            transcript = transcript_list.find_transcript(['zh-TW', 'zh-HK', 'zh-CN', 'en'])
            data = transcript.fetch()
            return " ".join([t['text'] for t in data])
        except Exception as e2:
            logging.warning(f"Could not get transcript for {video_id}: {e2}")
            return None

def summarize_with_transcript(title, transcript):
    prompt = f"æ¨™é¡Œï¼š{title}\nå­—å¹•å…§å®¹ï¼š{transcript[:8000]}" if transcript else f"æ¨™é¡Œï¼š{title}"
    sys_msg = """æ‚¨æ˜¯ä¸€ä½è³‡æ·±çš„é¦–å¸­è²¡ç¶“åˆ†æå¸«ã€‚è«‹é‡å°æä¾›çš„å½±ç‰‡æ¨™é¡ŒåŠå­—å¹•å…§å®¹ï¼Œæ’°å¯«ä¸€ä»½ç²¾ç…‰çš„åˆ†æã€‚
    è«‹ç›´æ¥ä»¥ã€Œé‡é»åˆ—è¡¨ (Point Form)ã€è¼¸å‡ºï¼Œåš´ç¦ä½¿ç”¨ä»»ä½•å°æ¨™é¡Œï¼ˆå¦‚ã€Œæ ¸å¿ƒè§€é»ã€ã€ã€Œé—œéµç´°ç¯€ã€ã€ã€ŒæŠ•è³‡è©•ä¼°ã€ç­‰ï¼‰ã€‚
    å…§å®¹æ‡‰å…·é«”æè¿°å½±ç‰‡ä¸­çš„é—œéµæ•¸æ“šã€å¸‚å ´å‹•æ…‹ã€æ˜ç¢ºçš„æŠ•è³‡åƒ¹å€¼æˆ–è¶¨å‹¢é æ¸¬ã€‚è«‹å‹™å¿…æåˆ°å½±ç‰‡ä¸­å…·é«”çš„æ•¸å­—ã€æ¨™çš„åç¨±ã€æˆ–ç‰¹å®šè§€é»ï¼Œçµ•å°é¿å…å¦‚ã€Œå½±ç‰‡æåˆ°äº†æ•¸æ“šã€ã€ã€Œåˆ†æäº†å¸‚å ´è¶¨å‹¢ã€ç­‰æ¦‚æ‹¬ã€ç©ºæ³›ä¸”ä¸å…·å¯¦è³ªå…§å®¹çš„æè¿°ã€‚
    è«‹ä½¿ç”¨æ­£å¼ç¹é«”ä¸­æ–‡ï¼Œç¸½å­—æ•¸æ§åˆ¶åœ¨ 500 å­—ä»¥å…§ã€‚è‹¥åŸå§‹å…§å®¹ç‚ºè‹±æ–‡ï¼Œè«‹å‹™å¿…ç¿»è­¯ä¸¦ä»¥æµæš¢çš„ä¸­æ–‡æ’°å¯«ã€‚è‹¥ç„¡å­—å¹•ï¼Œè«‹æ ¹æ“šæ¨™é¡Œé€²è¡Œæ¨ç†ä¸¦è¨»æ˜ã€Œï¼ˆæ ¹æ“šæ¨™é¡Œæ·±åº¦æ¨æ¼”ï¼‰ã€ã€‚"""
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "system", "content": sys_msg}, {"role": "user", "content": prompt}],
            timeout=45
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"Error summarizing {title}: {e}")
        return title

def is_within_2_weeks(time_str):
    if not time_str: return True
    time_str = time_str.lower()
    if any(x in time_str for x in ["minute", "hour", "day", "åˆ†é˜", "å°æ™‚", "å¤©"]):
        match = re.search(r'(\d+)', time_str)
        return int(match.group(1)) <= 14 if match and ("day" in time_str or "å¤©" in time_str) else True
    if any(x in time_str for x in ["week", "é€±", "å‘¨"]):
        match = re.search(r'(\d+)', time_str)
        return int(match.group(1)) <= 2 if match else True
    return False

def search_youtube(keyword):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}
    url = f"https://www.youtube.com/results?search_query={keyword}&sp=EgQIBBAB"
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # Handle cases where ytInitialData might be missing or structured differently
        match = re.search(r'var ytInitialData = (\{.*?\});', resp.text)
        if not match:
            logging.warning(f"Could not find ytInitialData for keyword: {keyword}")
            return []
            
        data = json.loads(match.group(1))
        
        # Navigate safely through the JSON structure
        try:
            items = data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']
        except KeyError:
            logging.warning(f"Unexpected JSON structure for keyword: {keyword}")
            return []

        videos = []
        for item in items:
            if 'itemSectionRenderer' in item:
                for content in item['itemSectionRenderer']['contents']:
                    if 'videoRenderer' in content:
                        v = content['videoRenderer']
                        t_str = v.get('publishedTimeText', {}).get('simpleText', '')
                        
                        # Basic data extraction
                        video_id = v.get('videoId')
                        title = v.get('title', {}).get('runs', [{}])[0].get('text', 'No Title')
                        channel = v.get('longBylineText', {}).get('runs', [{}])[0].get('text', 'No Channel')

                        if is_within_2_weeks(t_str):
                            videos.append({
                                'id': video_id, 
                                'title': title, 
                                'channel': channel, 
                                'time': t_str
                            })
        return videos
    except Exception as e:
        logging.error(f"Error searching YouTube for '{keyword}': {e}")
        return []

def update_website(all_records):
    # Keep only most recent 15 for the schedule
    recent = all_records[-15:]
    recent.reverse() # Show newest first in schedule
    
    try:
        if not os.path.exists(WEBSITE_PATH):
            logging.error(f"Website file not found at {WEBSITE_PATH}")
            return

        with open(WEBSITE_PATH, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        js_data = []
        for r in recent:
            # Metadata and Title formatting
            word_count = len(r.get('summary', ''))
            meta_header = f"ã€â±ï¸ {r.get('time', 'æœªçŸ¥')} | ğŸ”„ {r.get('processed_at', 'æœªçŸ¥')} | ğŸ“ {word_count} å­—ã€‘"
            # Subject first
            display_content = f"ğŸ“Œ ä¸»é¡Œï¼š{r.get('title', 'ç„¡æ¨™é¡Œ')}\n\n{meta_header}\n\n{r.get('summary', 'ç„¡æ‘˜è¦')}"
            js_data.append({"summary": display_content, "channel": r.get('channel', 'æœªçŸ¥é »é“'), "url": f"https://www.youtube.com/watch?v={r['id']}"})
        
        json_str = json.dumps(js_data, ensure_ascii=False).replace('\\', '\\\\')
        # Use more robust replacement
        pattern = r'const videos = \[.*?\];'
        replacement = f"const videos = {json_str};"
        if re.search(pattern, html_content, flags=re.DOTALL):
            new_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
            with open(WEBSITE_PATH, 'w', encoding='utf-8') as f:
                f.write(new_html)
        else:
            logging.error("Could not find 'const videos = [...];' pattern in index.html")
    except Exception as e:
        logging.error(f"Error updating index.html: {e}")

def git_push():
    try:
        subprocess.run(["git", "add", "."], check=True, capture_output=True)
        subprocess.run(["git", "commit", "-m", f"Dashboard update {datetime.now().strftime('%H:%M')}"], check=True, capture_output=True)
        subprocess.run(["git", "push"], check=True, capture_output=True)
        logging.info("Git push successful")
    except subprocess.CalledProcessError as e:
        logging.warning(f"Git push failed: {e.stderr.decode() if e.stderr else e}")
    except Exception as e:
        logging.error(f"Git push error: {e}")

def supabase_sync(records):
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return
    headers = {
        "apikey": SUPABASE_ANON_KEY, 
        "Authorization": f"Bearer {SUPABASE_ANON_KEY}", 
        "Content-Type": "application/json", 
        "Prefer": "resolution=merge-duplicates"
    }
    data = [{"id": r['id'], "title": r['title'], "channel": r['channel'], "summary": r['summary'], "url": f"https://www.youtube.com/watch?v={r['id']}"} for r in records]
    try:
        resp = requests.post(f"{SUPABASE_URL}/rest/v1/youtube_clips", headers=headers, json=data, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        logging.error(f"Supabase sync failed: {e}")

def main():
    logging.info("AI YouTube Subscription Monitor V1 (Safe & Logged) Started...")
    seen_ids = set(load_json(SEEN_VIDEOS_FILE, []))
    all_records = load_json(RECORDS_FILE, [])
    
    # Load subscriptions once (or reload every cycle if dynamic updates needed)
    subscription_channels = get_subscriptions()
    logging.info(f"Loaded {len(subscription_channels)} channels to monitor.")
    
    while True:
        logging.info("Cycle started...")
        new_batch = []
        
        # Batch processing: Randomly select 50 channels to check per cycle to manage rate limits/time
        # Time complexity: 50 searches * ~2s = 100s + summary time. 
        # Summary time for 5 videos = 5 * 10s = 50s. Total < 3 mins. Safe for 5 min sleep.
        if not subscription_channels:
             logging.warning("No subscriptions loaded. Please check è¨‚é–±.txt")
             current_batch_subs = []
        else:
            current_batch_subs = random.sample(subscription_channels, min(50, len(subscription_channels)))
        
        if current_batch_subs:
            logging.info(f"Checking {len(current_batch_subs)} channels in this cycle: {current_batch_subs[:5]}...")

        for sub_name in current_batch_subs:
            # Search for the channel to find recent videos
            results = search_youtube(sub_name)
            
            for vid in results:
                # Basic fuzzy matching for channel name to ensure it's the right channel
                if sub_name.lower() not in vid['channel'].lower() and vid['channel'].lower() not in sub_name.lower():
                    continue

                if vid['id'] in seen_ids: continue
                
                logging.info(f"Found new video from {vid['channel']}: {vid['title']}")
                transcript = get_transcript(vid['id'])
                vid['summary'] = summarize_with_transcript(vid['title'], transcript)
                if not transcript:
                    vid['summary'] = "âš ï¸âš ï¸âš ï¸ã€æ³¨æ„ï¼šç„¡å­—å¹•ï¼Œä»¥ä¸‹å…§å®¹ç‚º AI çœ‹æ¨™é¡Œèªªæ•…äº‹ï¼Œåƒ…ä¾›åƒè€ƒã€‘âš ï¸âš ï¸âš ï¸\n\n" + vid['summary']
                vid['processed_at'] = datetime.now().strftime('%m-%d %H:%M')
                new_batch.append(vid)
                seen_ids.add(vid['id'])
                
                # Send to Telegram (HTML Mode with escaping)
                word_count = len(vid['summary'])
                safe_title = html.escape(vid['title'])
                safe_summary = html.escape(vid['summary'])
                
                tg_msg = f"<b>ğŸ“Œ ä¸»é¡Œï¼š{safe_title}</b>\n\n" \
                         f"ğŸ“º <b>é »é“</b>: {html.escape(vid['channel'])}\n" \
                         f"â±ï¸ <b>æ™‚é–“</b>: {vid.get('time', 'æœªçŸ¥')}\n" \
                         f"ğŸ”„ <b>æŠ“å–</b>: {vid.get('processed_at', 'æœªçŸ¥')}\n" \
                         f"ğŸ“ <b>å­—æ•¸</b>: {word_count}\n\n" \
                         f"{safe_summary}\n\n" \
                         f"ğŸ”— <a href='https://www.youtube.com/watch?v={vid['id']}'>è§€çœ‹å½±ç‰‡</a>"
                send_telegram_msg(tg_msg)

                if len(new_batch) >= 5: break
            if len(new_batch) >= 5: break

        if new_batch:
            all_records.extend(new_batch)
            all_records = all_records[-200:] # Keep last 200 records locally
            save_json(RECORDS_FILE, all_records)
            save_json(SEEN_VIDEOS_FILE, list(seen_ids))
            supabase_sync(new_batch)
            logging.info(f"Recorded {len(new_batch)} new videos.")
            
        update_website(all_records)
        git_push()
        
        logging.info("Cycle finished. Sleeping 5m...")
        time.sleep(300)

if __name__ == "__main__":
    main()
